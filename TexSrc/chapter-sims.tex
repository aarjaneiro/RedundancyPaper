\chapter{Simulation Study}\label{ch:simulation-study}


\section{ParallelQueue Package}\label{sec:parallelqueue-package}
%======================================================================
In order to generate and study parallel queueing processes, few trivial options currently exist.
Moreover, while there exist some discrete event simulation (DES) frameworks which indeed focus on queueing networks, they currently tend not to permit the simultaneous study of asynchronous, redundancy-based schemes~\cite{noauthor_ciwpythonciw_nodate}.
In order to visualize and analyse the large class of queueing systems within this paradigm~\cite{shneer_large-scale_2020,cruise_stability_2020}, I introduced a novel module for Python which is currently available on PyPi: \textit{ParallelQueue} extending the DES package \textit{SimPy}.

The package currently allows for the studying of parallel systems with or without redundancy as well as with the option of allowing thresholds to be implemented in either case.
Moreover, the package allows one to specify any inter-arrival and service time distribution as well as their own \lstinline{Monitor}s, being a class which can gather data from the ongoing simulation to be distributed back to the user upon the completion of a simulation.
In particular, the \lstinline{Monitor}s are currently configured to collect data upon arrival, routing, and job completion as demonstrated by Figure~\ref{fig:API}.

Take Listing~\ref{red} and Figure~\ref{fig:redpic} for example, which permits one to simulate a Redundancy-2 queueing system with 100 queues in parallel for 1000 units of time while returning the total queue counts over time (which are updated upon a change in queue count).


    \begin{lstlisting}[label={fig:red}, caption={Python code using \textit{ParallelQueue} to simulate a Redundancy-2 System.}, language=Python, style=mystyle]
#!/usr/bin/python3
from parallelqueue.base_models import RedundancyQueueSystem
from parallelqueue.monitors import TimeQueueSize
import random

sim = RedundancyQueueSystem(
                    maxTime=1000.0,parallelism=100, seed=1234,
                    d=2, Arrival=random.expovariate,
                    AArgs=9, Service=random.expovariate,
                    SArgs=0.08, Monitors = [TimeQueueSize])
# Note RedundancyQueueSystem is a ParallelQueueSystem wrapper
sim.RunSim()
totals = sim.MonitorOutput["TimeQueueSize"]
    \end{lstlisting}

\begin{figure}
    \centering
    \includegraphics[scale=0.8]{redundancy}
    \caption{Plot using \lstinline{totals} of Figure~\ref{fig:red}}
    \label{fig:redpic}
\end{figure}

Remarkably, the simulation itself is performed speedily on consumer hardware despite the size of the system as demonstrated in Listing~\ref{fig:lstlisting2}.
    \begin{lstlisting}[label={fig:lstlisting2},style=mystyle, caption={Runtime Statistics Using the Master ParallelQueue Branch}]
CPU times: user 2.02 s, sys: 9.57 ms, total: 2.03 s
Wall time: 2.05 s
Intel i5-8250U (8) @ 3.400GHz
    \end{lstlisting}


Altogether, this makes the package easy to parallelize with and thus to compare systems of different sizes
and with large running-times.
While currently not implemented in any development branch of \textit{ParallelQueue}, the base Python package
\textit{multiprocessing} is used in throughout this paper when simulating for the same system across parameters.
In general, the main caveat when processing many models is that the storage of the simulation results can quickly begin
to consume storage;
when processing many models, therefore ensure that they are saved (e.g., using \textit{pickle}) and removed from
the local environment when doing analysis.

In terms of development, the models implemented in the \textit{base\_models} module use the framework established in~\ref{ch:model-specification}.
That is, modelling redundancy, a hyperedge of sorts is generated whence the dispatcher
receives a job to be cloned.
This hyperedge then exists for the duration of time for which the replica class is in the system and is defined in such
a way that \textit{Monitor} class objects can interact with them in order to acquire data.
In Python, such a data structure can be implemented rather easily by employing the \textit{Dict} type which defines
a keyed set of values.
By keying based on the job arrivals (before cloning), a unique set of marks can be retrieved for the set by simply using
the \textit{Dict} object as a reference.

\begin{figure}
    \centering
    \includegraphics[scale=0.7]{pq}
    \caption{Overview of the ParallelQueue API}
    \label{fig:API}
\end{figure}


\section{Results}\label{sec:results}

\subsection{Methodology}\label{subsec:methodology}
First, we examine each model in terms of their respective performance in $E(T)$, the expected time each job spends in the system.
As Figure~\ref{fig:img} shows, for a load $\rho \triangleq \frac{\lambda}{\mu} = 0.5$ by taking $\mu=1, \lambda = 0.5$ (we will assume $\mu \equiv 1$ for the
rest of the simulations), Redundancy(2)) and Threshold(2,2) policies are rather alike with low loads as $N \rightarrow \infty$. This is to be expected, of course, given that even such a low threshold is unlikely to be exceeded with the processors acting faster than arrivals on average. Ignoring cancellation costs, this clearly demonstrates how utilizing otherwise dormant queues comes to benefit the system's performance. Note that the figure is generated with the \textit{same} seed generation scheme for 30 different seeds per iteration, making the overlap a product of the two accessing random numbers from the system at the same points in time (in their respective simulations).

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{attimes} %
    \caption{Comparisons of Systems: Values are averaged over 30 independent iterations each, running for $t=1000$)}
    \label{fig:img}
\end{figure}

As~\cite{gardner_redundancy-d_2017} show, a Redundancy($d$) system is asymptotically stable if and only if $\rho < 1 $. Given that, in premise, Threshold($r,d$) models are more or less a superclass of Join-The-Shortest queue and Redundancy($d$) (trivially with rising $r$  implying no threshold exists and thus copies should always be made as in the case of Redundancy($d$), it is perhaps most interesting to examine if Threshold models are better able to handle high-load environments. Proving the superclass property in terms of Redundancy is relatively easy and is done in Lemma~\ref{sup}. By contrast, after merely setting $r \equiv 0$, we get $ \mathcal{D}_{\text{Thresh(0,d)}} \overset{d}{=} \mathcal{D}_{\text{JSQ(d)}}$ by definition. To prove the lemma, we first must introduce the concepts of stochastic domination and coupling.
\begin{definition}[Stochastic Domination]
    \\~\\
    $\mathbf{X}_{1}^{(N)}$ is said to dominate $\mathbf{X}_{2}^{(N)}$, using the notation $\leq_{st}$, when ~\cite{bramson_asymptotic_2012}
    \[\mathbf{X}_{1}^{(N)}\leq_{st}\mathbf{X}_{2}^{(N)} \iff\# X_{1,i} \leq \# X_{2,i}  \quad \forall i \in [N] \quad P-a.s.\]
\end{definition}

\begin{definition}[Coupled Process]
    \label{coupled}
    \\~\\
    For stochastic processes $X_{t} \in Pr(\Omega_{x}, \mathcal{F}_{x}, \mathcal{F}_{t,x})$ and $X_{t} \in Pr(\Omega_{y}, \mathcal{F}_{y}, \mathcal{F}_{t,y})$.
    $X_{t}$ and $Y_{t}$ are said to be coupled over the probability space $Pr(\Omega, \mathcal{F}, \mathcal{F}_{t})$ if there exist
    $\hat X_{t}, \hat Y_{t} \in Pr(\Omega, \mathcal{F}, \mathcal{F}_{t})$ such that $\hat X_{t} \overset{d}{=}X_{t}$ and $\hat Y_{t} \overset{d}{=}Y_{t}$ \cite{bramson_asymptotic_2012}.
\end{definition}

\begin{lemma}
    \label{sup}
    For $\mathbf{X}^{(N)}$ being a system of queues such that $\rho < 1$, \[\mathcal{D}_{\text{Thresh(r,d)}}\mathbf(X^{(N)}) \overset{r \rightarrow \infty}{\rightsquigarrow} \mathcal{D}_{\text{Red(d)}}\mathbf(X^{(N)}).\]
\end{lemma}
\textit{Proof}:

To show the sequence of routing algorithms to be convergent, we will construct a coupling in such a manner that $\forall r \in \mathbb{R}^{+}$, the system is stochastically dominated by another which is made to be finite~\cite{baccelli_elements_2003} . To do so, for $\mathbf{X}_{1}^{(N)}$ under $\text{Thresh}(r,d)$, denote the first arrival as $T_{\xi_{1}}$, at which time it is the case that a selection set, $\nu \subset [N] $, is prescribed wherein there exists $\hat X \in \{X_{i}\}_{i \in \nu} $ such that $ \# (\hat X) > r $. Thus, for  $ t \in [0,T_{\xi_{1}})$, we have $\mathcal{D}_{\text{Thresh(r,d)}}\mathbf(X^{(n)}) = \mathcal{D}_{\text{Red(d)}}\mathbf(X^{(n)})$. For clarity, let us now consider $\mathbf{Y}^{(N)}$ to be a copy of  $\mathbf{X}^{(N)}$ such that they are independent, identical in distribution and in terms of the marks of \textit{arrival process and job-size draws} along with the queues parsed (as was similarly done in~\cite{bramson_asymptotic_2012} in Lemma 4.1 using Definition \ref{coupled}). Effectively, the only difference being left between these copies is $r$ changing which queues receive clones and thus, too, the mark of queue-dependency. At time $T_{\xi_{1}}$ we clearly have $\mathcal{D}_{\text{Red(d)}}\mathbf(X^{(n)}) = \mathcal{D}_{\text{Thresh(r,d)}}\mathbf(Y^{(n)})$ due to there being no existing jobs for which the threshold would perclude cloning in the case of Thresh($r,d$).
As such, we also have $\mathcal{D}_{\text{Thresh}(r,d)}(Y^{(N)}(T_{\xi_{1}}))\leq_{st}\mathcal{D}_{\text{Red}(d)}(X^{(N)}(T_{\xi_{1}}))$.

Now, assume $\rho < 1$, giving us $ \# Y_{i}  < \infty \quad \forall t \in \mathbb{R}^{+}$ with probability 1~\cite{gardner_redundancy-d_2017}. As such, we have that $\forall r \in \mathbb{R}^{+}$ there exists $ \xi_{1}(r) $ such that $ P_{r}(\mathbf{X}^{(N)}(t) = \mathbf{Y}^{(N)}(t) |t \in [0,T_{\xi_{1}}) \right) = 1$ $P_{r}(A) := P(\mathcal{D}_{M(r)}(A))$ wherein $M$ denotes the routing algorithm of process $A$. Note that $\xi_{1} (r)$ is monotone increasing in $r$. Letting $r \rightarrow \infty \Rightarrow \xi_{1} \rightarrow \infty$, we then have $\mathbf{X}^{(N)}(t) =  \mathbf{Y}^{(N)}(t) \quad \forall t \in \mathbb{R}^{+}$ in terms of distribution (i.e., as the result holds $\forall t \in \mathbb{R}^{+}$ as $r\rightarrow \infty$), implying the required weak convergence from below for $\mathcal{D}$ in law over system  $\mathbf{X}^{(N)}(t)$.  \qed

In order to actually test the hypothesized results with respect to independence, we employ the joint Hilbert-Schmidt Independence Criterion (HSIC)~\cite{jointindep} to test the independence of queue counting processes $\{\#X_{n}(t)\}_{n \in [N]}$ in a Monte Carlo-styled
simulation consisting of $N_{\text{MC}}$ simulations for the queueing process $\mathbf{X}^{(N)}$.
\begin{definition}[HSIC Test for Two Processes]

    \label{hsichat}
    Following~\cite{nonstat}, for processes $X_{t}$ and $Y_{t}$ with common time domain $t \in T$, denote by $\kappa_{x}, \kappa_{y}$ the kernels which
    map each process to a seperable reproducing kernel Hilbert space ($\mathcal{H}_{x}, \mathcal{H}_{y}$, respectively).
    Define HSIC to be the $\mathcal{H}_{x} \otimes \mathcal{H}_{y}$ norm of form:
    \[\text{HSIC}(\mathcal{H}_{x},\mathcal{H}_{y}) = \|\mu_{XY} - \mu_{X} \otimes \mu_{Y}\|^{2}_{\mathcal{H}_{x} \otimes \mathcal{H}_{y}},\]
    where $\mu_{Z}$ denotes a mean embedding of process $Z$ onto space $\mathcal{H}_{Z}$.
    \\~\\
    In order to test the hypotheses of
    \[H_{0}: P_{XY} = P_{X}P_{Y}, H_{a}: P_{XY} \not = P_{X}P_{Y},\]
    where $P_{Z}$ denotes the marginal distribution of $Z$,~\cite{nonstat} suggest a resampling procedure.
    Specifically, for any estimator of $\text{HSIC}(XY)$, ${H_{\text{HSIC}}}(XY)$, sample a common set of $N_{\text{time}}$ times, $\{\tau_{\ell}\}_{ell \leq N_{\text{time}}} \subset T$, and calculate \[K :=\{H_{\text{HSIC}}(X_{(\{\tau_{N_{\text{time}}}\})}Y_{\gamma(\tau_{N_{\text{time}}})}))\}_{\gamma \in \Gamma}\] where $X_{(\{\tau_{N_{\text{time}}}\})}$ denotes $X_{t}$ restricted to the sampled times $\{\tau_{N_{\text{time}}}\}$ and $\Gamma$ is a set of permutations on these $N_{\text{time}}$ samples ordered such that $K_{i} \leq K_{i+1}$.
    After taking $c_{\alpha} :=  K_{(1-\alpha)N_{\text{time}}}$, the null hypothesis is rejected if $H_{\text{HSIC}}(XY) > c_{\alpha}$.
\end{definition}

%Specifically, we utilize an extension of HSIC testing for non -stationary processes described in~\cite{nonstat}, wherein for some common set of $N_{\text{time}}$ sampled times, $\{\tau_{\ell}\}_{ell \leq N_{\text{time}}}$,
Unlike the procedure utilized by~\cite{nonstat}, however, we adopt a modification described in~\cite{jointindep} to test for joint independence across $N$ processes.
While retaining the time sampling procedure of~\cite{nonstat}, we are instead concerned with testing upon the so-called $d\text{HSIC}$.

\begin{definition}[$d$HSIC]

    \label{dhsic}
    For a set of $d$ stochastic processes $\mathcal{X} = \{X_{i}\}_{i \leq d}$, define $d\text{HSIC}$ as:
    \[d\text{HSIC}(\mathcal{X}) := \|\mu_{\bigotimes_{i \leq d}(P_{X_{i}})} - \mu_{\{P_{X_{i}}\}_{i \leq d}}\|_{\bigotimes_{i \leq d}\mathcal{H}_{X_{i}}}\]
\end{definition}

Much like the procedure described in Definition~\ref{hsichat}, an estimator and critical value can also be calculated in order to test for independence.
The particular means of estimation used in this research is outlined in Sections 4.1, 4.2, and B.5 of~\cite{jointindep} and is
implemented in the codebase used for our simulations as outlined in Appendix~\ref{sec:sic}.

\subsection{Simulation Results}\label{subsec:simulation-results}

\begin{table}
    \centering
    \caption{$d$HSIC for Varying $N$}
    \label{tab:reg}
    \begin{tabular}{|c|c|c|c|}
\hline
$\rho$ & $N$ & Thresh & $d$HSIC \\
\hline
\hline
 & 2 & 1.00e+00 & 1.00e+00 * \\
 & 5 & 7.62e-04 & 6.43e-01  \\
0.8 & 25 & 3.08e-13 & 3.43e-01  \\
 & 50 & 4.63e-16 & 9.06e-01  \\
 & 100 & 4.91e-16 & 8.52e-01  \\
\hline
 & 2 & 1.00e+00 & 1.00e+00 * \\
 & 5 & 4.23e-04 & 4.85e-01  \\
0.9 & 25 & 1.06e-12 & 4.59e-01  \\
 & 50 & 3.73e-16 & 5.79e-02  \\
 & 100 & 4.96e-16 & 1.64e-01  \\
\hline
 & 2 & 1.00e+00 & 5.13e-01  \\
 & 5 & 5.78e-04 & 5.19e-01  \\
0.99 & 25 & 6.28e-12 & 9.92e-01  \\
 & 50 & 4.39e-16 & 9.10e-01  \\
 & 100 & 4.21e-16 & 8.70e-01  \\
\hline
    \end{tabular}

    \begin{tabular}{p{10cm}}
        \textit{Note:} Sampled times comprise every 5th value of the set $[100,200]$.
        Data are drawn from 12 simulations run for each $N$ and are used to derive the test at $\alpha = 0.05$.
    \end{tabular}
\end{table}


\begin{table}
    \centering
    \caption{$d$HSIC Test for Large $t$}
    \label{tab:longtime}
    \begin{tabular}{|c|c|c|c|}
\hline
$\rho$ & $N$ & Thresh & $d$HSIC \\
\hline
\hline
 & 2 & 1.00e+00 & 1.00e+00 * \\
 & 5 & 8.80e-04 & 2.71e-01  \\
0.8 & 25 & 1.36e-12 & 1.68e-01  \\
 & 50 & 4.70e-16 & 4.07e-01  \\
 & 100 & 5.14e-16 & 8.86e-01  \\
\hline
 & 2 & 1.00e+00 & 1.00e+00 * \\
 & 5 & 7.60e-04 & 7.52e-01  \\
0.9 & 25 & 3.88e-13 & 7.98e-02  \\
 & 50 & 2.82e-16 & 3.39e-01  \\
 & 100 & 3.71e-16 & 4.27e-01  \\
\hline
 & 2 & 1.00e+00 & 1.00e+00 * \\
 & 5 & 8.77e-04 & 2.69e-01  \\
 0.99 & 25 & 4.54e-11 & 8.32e-01  \\
 & 50 & 4.02e-16 & 5.03e-01  \\
 & 100 & 5.00e-16 & 3.99e-01  \\
\hline
    \end{tabular}

    \begin{tabular}{p{10cm}}
        \textit{Note:} Sampled times comprise every 5th value of the set $[900,1000]$.
        Data are drawn from 12 simulations run for each $N$ and are used to derive the test at $\alpha = 0.05$.
    \end{tabular}
\end{table}







